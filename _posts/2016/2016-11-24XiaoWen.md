---
layout: post
title: "Crowdsourcing the Pronunciation of Out-of-vocabulary Words"
author: "Wen Xiao"
supervisors: "Professor Gerald Penn"
category: "Natural Language Processing"
permalink: /natural-language-processing/out-of-vocabulary-words
year: "2016"
---

Introduction
============

For both automatic speech recognizers (ASR) and text-to-speech
synthesizers (TTS), out-of-vocabulary (OOV) words account for a
significant number of errors. These words are not the words we rarely
use, but the words that are absent from the lexicon of the ASR or TTS
system. They usually represent important semantic content such as names,
locations and compound words. In both ASR and TTS systems, it is very
hard to recognize this kind of word. In the case of TTS, the
pronunciation of the word is unknown, so a guess would be based on
general ’letter-to-sound’ or ’grapheme-to-phoneme’ rules, which are
error prone. This is especially the case if the provenance of the word
is unknown, or if the writing system is more logographically
constructed.

An alternative solution involves asking a human how an unknown word
should be pronounced; this technique is known as “crowdsourcing”.
Crowdsourcing has been explored as a solution for this problem before by
Rutherford et al. [1], but their approach used the tool on a very
small scale. Our approach simply uses a larger number of speakers (101)
and an acoustic model by constructing phone lattices and submitting
candidate pronunciation paths to a simple weighted voting algorithm,
which combines results across crowdsource workers. The evaluation of our
approach will be the focus of this paper.

Construction
============

To begin, an acoustic model is built using the Kaldi toolkit [2]. Each
possible phone is treated as a 1-phone word, with a trivial lexicon that
maps every phone to itself, and a flat 0-gram language model. By using
the Kaldi toolkit, we obtained the 500 (or fewer) best pronunciations
for each speaker for a given word, ranked them according to the score
assigned by Kaldi, and calculated the word’s overall score. The selected
pronunciation for the unknown word is the one with the highest score. To
improve accuracy, among the 101 crowdsourcing workers, only 43 were
chosen based on the result of a qualifying test. To simplify the
evaluation process, we selected words with known pronunciations as
putative OOV words for the purposes of this experiment. Thus, 100 words
were sampled from the CMU pronunciation dictionary based on word
frequency, word length, etc. Figure 1 shows the sampled word list. We
then generated a pronunciation for them using the aforementioned method,
and compared the generated pronunciation with those in the CMU
dictionary.

<img style="margin:50" src="{{ site.baseurl }}/assets/xw_words.png"/>

<p style="text-align:center">Figure 1: The experimental OOV lexicon.</p>

Evaluation
==========

There are many ways to evaluate the proposed method. The first approach
is simply to regard the CMU pronunciation dictionary as the standard and
measure the deviation of our approach from that. Here we use the
Levenshtein distance as the measurement of the deviation. Both
macro-averaged Levenshtein distance and the micro one are near 0.2,
which shows the pronunciations of our approach and the CMU dictionary do
not differ too much. 

The second approach is to regard the CMU
pronunciation dictionary as the standard but, as opposed to the first
method, then examine the rank of CMU pronunciations within the 500-best
lists of the crowdsource workers that we obtained before. This is a
non-parametric cousin of a data likelihood computation in a
probabilistic model. In the case of 20 of the 100 words, our method
selected a pronunciation found for that word in the CMU dictionary
(Figure 2 shows the distribution of ranks and their medians for each
word). The other 80 words were not found in the CMU dictionary.

<img style="margin:50" src="{{ site.baseurl }}/assets/xw_cmu_rates.png"/>
Median, minimum and maximum ranks of the CMU
pronunciations

The third way is another kind of crowdsourcing for evaluation, in which
we consider all the pronunciations as candidates, and ask crowdsource
workers to evaluate them. We evaluated our selected pronunciations and
the CMU dictionary’s pronunciations in two ways: by asking crowdsource
workers to transcribe synthesized pronunciations, and by asking
crowdsource workers to choose which of two alternative pronunciations
(ours and one of the CMU pronunciations) is better. The transcriptions
of CMU pronunciations had a modestly lower average Levenshtein distance
than ours. As for the selection task, workers were asked to decide
whether:

\(1) The first of two prompts was better

\(2) The second was better, or

\(3) The two were about the same.

We permuted the order in which our pronunciation and the CMU
pronunciation were presented, to avoid presentation bias. The CMU
pronunciation received more votes on 42 word comparisons, while the
experimental pronunciation received more votes on 38 word comparisons.
There were no ties; the other 20 words are those for which the two
pronunciations were segmentally identical. In general, this is still a
positive showing for a method that uses no letter-to-sound productions.
The crowd does very well for itself in aggregate, even though no
individual worker does. Although our pronunciations are often different,
they are accepted widely by the crowd — more than CMU’s almost half of
the time.

Conclusion
==========

The proposed method for selecting pronunciations of OOV words has proved
to be competitive with the phone sequences found in the CMU
pronunciation dictionary, across several different distributional
criteria, even though there is not a great deal of overlap between the
two sets of pronunciations. In the future, we could improve our results
by using higher sampling rates and sizes. We could also introduce stress
accent prediction, which, at the time of writing, hampers our ability to
fairly evaluate against the CMU dictionary.

References
---------

1.  F. P. Attapol T. Rutherford and F. Beaufays, Pronunciation learning for named-entities through crowdsourcing. Proceedings of Interspeech 2014, 2014.

2. D. Povey and et al.,  “The kaldi speech recognition toolkit,”  Dec. 2011. IEEE Catalog No.: CFP11SRW-USB.

